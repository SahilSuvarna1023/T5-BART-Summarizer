{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAVXtvIJBRjTK+q0HSmSyy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SahilSuvarna1023/T5-BART-Summarizer/blob/main/Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, BartForConditionalGeneration, BartTokenizer\n",
        "import evaluate\n",
        "from evaluate import load"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4tFi_IS319_",
        "outputId": "e1632dd1-4e6f-4472-d086-a8f977e692a6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_dataset(df):\n",
        "    # Drop missing values\n",
        "    df = df.dropna()\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates()\n",
        "    # Trim whitespace\n",
        "    df = df.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "    return df"
      ],
      "metadata": {
        "id": "02nIg04i34_n"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path, sample_size=50):\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Columns in dataset:\", df.columns.tolist())  # Debugging line\n",
        "\n",
        "    # Ensure correct column name\n",
        "    if 'article' in df.columns:\n",
        "        df = df.rename(columns={'article': 'text'})\n",
        "    else:\n",
        "        raise KeyError(\"Expected column 'article' not found in dataset. Please check column names.\")\n",
        "\n",
        "    df = clean_dataset(df)\n",
        "    df = df.sample(n=sample_size, random_state=42)  # Select 50 random samples\n",
        "    return df"
      ],
      "metadata": {
        "id": "5HJ6QBKM4Aov"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text_t5(text, model, tokenizer, max_length=150):\n",
        "    input_text = \"summarize: \" + text\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=max_length, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def summarize_text_bart(text, model, tokenizer, max_length=150):\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=max_length, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def evaluate_summaries(references, predictions):\n",
        "    rouge = load(\"rouge\")\n",
        "    results = rouge.compute(predictions=predictions, references=references)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "-ckurJGG4BYf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load dataset (only 50 samples)\n",
        "    dataset_path = \"/content/test.csv\"  # Update with correct column names\n",
        "    df = load_dataset(dataset_path, sample_size=50)\n",
        "\n",
        "    # Load models\n",
        "    t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "    t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", legacy=False)\n",
        "    bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "    bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # Summarize and evaluate\n",
        "    df['t5_summary'] = df['text'].apply(lambda x: summarize_text_t5(x, t5_model, t5_tokenizer))\n",
        "    df['bart_summary'] = df['text'].apply(lambda x: summarize_text_bart(x, bart_model, bart_tokenizer))\n",
        "\n",
        "    # Evaluate with Rouge\n",
        "    rouge_scores = evaluate_summaries(df['text'].tolist(), df['t5_summary'].tolist())\n",
        "    print(\"ROUGE Scores:\", rouge_scores)\n",
        "\n",
        "    # Save results\n",
        "    df.to_csv(\"/content/summarized_results.csv\", index=False)\n",
        "    print(\"Summarization complete. Results saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE4MEpld4GTT",
        "outputId": "90f950a0-a15f-40bc-ecdb-fe6998e2d451"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in dataset: ['id', 'article', 'highlights']\n",
            "ROUGE Scores: {'rouge1': 0.16387567273483772, 'rouge2': 0.1414180912330464, 'rougeL': 0.15038435459887795, 'rougeLsum': 0.15125620515092308}\n",
            "Summarization complete. Results saved.\n"
          ]
        }
      ]
    }
  ]
}